import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Load the elephant calls dataset
df = pd.read_csv("paste.txt")
print("Dataset shape:", df.shape)
print("\nFirst few rows:")
print(df.head(3))

# Examine the data types and missing values
print("\nMissing values per column:")
missing_data = df.isnull().sum()
print(missing_data[missing_data > 0].head(10))

print("\nUnique values in key columns:")
print("Call_Type:", df['Call_Type'].unique())
print("Context:", df['Context'].unique())
print("Distance range:", df['Distance'].min(), "to", df['Distance'].max())
print("Quality range:", df['Quality'].min(), "to", df['Quality'].max())

# Clean the dataset - focus on meaningful features with sufficient data
# Remove rows where key features are missing
key_features = ['Distance', 'Quality', 'Call_Type', 'CallerAge', 'Elicitor1Age']
df_clean = df.dropna(subset=['Call_Type', 'Quality'])  # Keep essential features

# Handle Distance column (has 'NA' strings)
df_clean = df_clean[df_clean['Distance'] != 'NA'].copy()
df_clean['Distance'] = pd.to_numeric(df_clean['Distance'], errors='coerce')
df_clean = df_clean.dropna(subset=['Distance'])

print(f"\nCleaned dataset shape: {df_clean.shape}")
print("Remaining Call_Type distribution:")
call_type_dist = df_clean['Call_Type'].value_counts()
print(call_type_dist)

# Check if we have enough diversity in the data
if len(call_type_dist) == 1:
    print(f"\nWARNING: Only one call type '{call_type_dist.index[0]}' found!")
    print("This will result in a single-node tree. Consider:")
    print("1. Using different features")
    print("2. Using a different target variable") 
    print("3. Checking data preprocessing")
    
    # Let's try using Context as target instead if Call_Type is uniform
    print(f"\nTrying Context as alternative target:")
    context_dist = df_clean['Context'].value_counts()
    print(context_dist.head())
    if len(context_dist) > 1:
        print("Using Context as target variable instead of Call_Type")
        df_clean['Target'] = df_clean['Context']
    else:
        print("Context also has limited diversity. Creating synthetic target based on Distance ranges.")
        # Create synthetic target based on distance quartiles
        df_clean['Target'] = pd.qcut(df_clean['Distance'], q=3, labels=['Near', 'Medium', 'Far'])
else:
    df_clean['Target'] = df_clean['Call_Type']

# A1: Equal Width Binning and Entropy Calculation
def equal_width_binning(series, bins=4):
    """
    Perform equal width binning on a continuous series
    
    Parameters:
    series: pandas Series - continuous data to bin
    bins: int - number of bins to create (default=4)
    
    Returns:
    pandas Series - binned categorical data with proper labels
    """
    try:
        # Use pd.cut with proper handling of edge cases
        binned = pd.cut(series, bins=bins, labels=[f'Bin_{i}' for i in range(bins)], 
                       include_lowest=True, duplicates='drop')
        # Convert to numeric codes for easier processing
        return pd.cut(series, bins=bins, labels=False, include_lowest=True, duplicates='drop')
    except ValueError as e:
        print(f"Binning error: {e}")
        # Fallback: create simple quantile-based bins
        return pd.qcut(series.rank(method='first'), q=bins, labels=False)

def entropy(series):
    """
    Calculate entropy of a categorical series
    H = -Σ(pi * log2(pi))
    
    Parameters:
    series: pandas Series - categorical data
    
    Returns:
    float - entropy value
    """
    # Remove NaN values and convert to array
    series_clean = pd.Series(series).dropna()
    if len(series_clean) == 0:
        return 0
    
    # Get value counts and probabilities
    counts = series_clean.value_counts()
    probabilities = counts / len(series_clean)
    
    # Calculate entropy, avoiding log(0)
    entropy_val = 0
    for p in probabilities:
        if p > 0:
            entropy_val -= p * np.log2(p)
    
    return entropy_val

# Apply binning to Distance (primary continuous feature)
print("\nApplying binning to Distance feature...")
df_clean['Distance_binned'] = equal_width_binning(df_clean['Distance'], bins=4)
print("Distance binning result:")
print(df_clean['Distance_binned'].value_counts().sort_index())

# Calculate entropy for Distance_binned
entropy_distance = entropy(df_clean['Distance_binned'])
print(f"\nA1 - Entropy of Distance_binned: {entropy_distance:.4f}")

# Also bin Quality for comparison
df_clean['Quality_binned'] = equal_width_binning(df_clean['Quality'], bins=4)
entropy_quality = entropy(df_clean['Quality_binned'])
print(f"A1 - Entropy of Quality_binned: {entropy_quality:.4f}")

# A2: Gini Index Calculation
def gini_index(series):
    """
    Calculate Gini index of a categorical series
    Gini = 1 - Σ(pj^2)
    
    Parameters:
    series: pandas Series - categorical data
    
    Returns:
    float - Gini index value
    """
    series_clean = pd.Series(series).dropna()
    if len(series_clean) == 0:
        return 0
    
    counts = series_clean.value_counts()
    probabilities = counts / len(series_clean)
    
    # Calculate Gini index
    gini_val = 1 - sum(p ** 2 for p in probabilities)
    return gini_val

gini_distance = gini_index(df_clean['Distance_binned'])
gini_quality = gini_index(df_clean['Quality_binned'])
print(f"\nA2 - Gini Index of Distance_binned: {gini_distance:.4f}")
print(f"A2 - Gini Index of Quality_binned: {gini_quality:.4f}")

# A3: Information Gain and Root Node Selection
def information_gain(df, feature, target):
    """
    Calculate information gain for a feature with respect to target
    IG = H(S) - Σ(|Sv|/|S| * H(Sv))
    
    Parameters:
    df: pandas DataFrame
    feature: str - feature column name
    target: str - target column name
    
    Returns:
    float - information gain value
    """
    # Create a clean dataframe for this calculation
    df_calc = df[[feature, target]].dropna()
    if len(df_calc) == 0:
        return 0
    
    # Calculate total entropy of target
    total_entropy = entropy(df_calc[target])
    
    # Calculate weighted entropy after split
    feature_values = df_calc[feature].unique()
    weighted_entropy = 0
    total_samples = len(df_calc)
    
    for value in feature_values:
        subset = df_calc[df_calc[feature] == value]
        if len(subset) > 0:
            weight = len(subset) / total_samples
            subset_entropy = entropy(subset[target])
            weighted_entropy += weight * subset_entropy
    
    # Information Gain = Total Entropy - Weighted Entropy
    info_gain = total_entropy - weighted_entropy
    return info_gain

# Create proper target variable - encode the target
le_target = LabelEncoder()
df_clean['Target_encoded'] = le_target.fit_transform(df_clean['Target'])
print(f"\nTarget encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}")
print(f"Target distribution: {pd.Series(df_clean['Target_encoded']).value_counts().sort_index()}")

# Calculate information gain for different features
features_to_test = ['Distance_binned', 'Quality_binned']
information_gains = {}

for feature in features_to_test:
    ig = information_gain(df_clean, feature, 'Target_encoded')
    information_gains[feature] = ig
    print(f"A3 - Information Gain of {feature}: {ig:.4f}")

# Find best feature for root node
best_root_feature = max(information_gains, key=information_gains.get)
print(f"\nA3 - Best root node feature: {best_root_feature} (IG: {information_gains[best_root_feature]:.4f})")

# A4: Flexible Binning Function with Overloading
def flexible_binning(series, bins=4, strategy='uniform', bin_type='equal_width'):
    """
    Flexible binning function with different strategies and default parameters
    
    Parameters:
    series: pandas Series - data to bin
    bins: int - number of bins (default=4)
    strategy: str - binning strategy ('uniform', 'quantile', 'kmeans')
    bin_type: str - type of binning ('equal_width', 'equal_frequency')
    
    Returns:
    numpy array - binned data as categorical codes
    """
    series_clean = series.dropna()
    
    if len(series_clean) == 0:
        return np.array([])
    
    try:
        if bin_type == 'equal_frequency':
            # Equal frequency binning using quantiles
            return pd.qcut(series_clean, q=bins, labels=False, duplicates='drop')
        else:  # equal_width
            if strategy == 'uniform':
                enc = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='uniform')
            elif strategy == 'quantile':
                enc = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='quantile')
            else:  # kmeans
                enc = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')
            
            result = enc.fit_transform(series_clean.values.reshape(-1, 1))
            return result.astype(int).flatten()
    except Exception as e:
        print(f"Binning failed with {strategy}, using fallback: {e}")
        # Fallback to simple equal width
        return pd.cut(series_clean, bins=bins, labels=False, duplicates='drop')

# Test flexible binning with different strategies
print("\nA4 - Testing flexible binning strategies:")
for strategy in ['uniform', 'quantile']:
    binned_result = flexible_binning(df_clean['Distance'], bins=4, strategy=strategy)
    print(f"Strategy {strategy}: {len(np.unique(binned_result))} unique bins created")

# Add more binned features
df_clean['CallerAge_binned'] = flexible_binning(df_clean['CallerAge'].fillna(df_clean['CallerAge'].median()), bins=3)
print("A4 - Added CallerAge_binned feature")

# A5: Build Custom Decision Tree Module
class CustomDecisionTree:
    """
    Custom Decision Tree implementation using Information Gain
    """
    
    def __init__(self, max_depth=5, min_samples_split=5, min_samples_leaf=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.tree = {}
        self.feature_names = []
        self.classes = []
    
    def fit(self, X, y):
        """
        Build the decision tree
        """
        self.feature_names = X.columns.tolist() if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]
        self.classes = np.unique(y)
        
        # Convert to DataFrame if needed
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=self.feature_names)
        if not isinstance(y, pd.Series):
            y = pd.Series(y)
            
        self.tree = self._build_tree(X, y, depth=0)
        return self
    
    def _build_tree(self, X, y, depth):
        """
        Recursively build the decision tree
        """
        # Base cases
        if (depth >= self.max_depth or 
            len(X) < self.min_samples_split or 
            len(y.unique()) == 1 or
            len(X) < 2 * self.min_samples_leaf):
            return {'class': y.mode().iloc[0] if len(y) > 0 else self.classes[0], 
                   'samples': len(y),
                   'distribution': y.value_counts().to_dict()}
        
        # Find best feature to split on
        best_feature, best_gain = self._find_best_split(X, y)
        
        if best_feature is None or best_gain <= 0:
            return {'class': y.mode().iloc[0] if len(y) > 0 else self.classes[0],
                   'samples': len(y),
                   'distribution': y.value_counts().to_dict()}
        
        # Create tree node
        tree_node = {
            'feature': best_feature, 
            'gain': best_gain,
            'samples': len(y),
            'distribution': y.value_counts().to_dict(),
            'children': {}
        }
        
        # Split data and create children
        feature_values = X[best_feature].unique()
        for value in feature_values:
            mask = X[best_feature] == value
            if mask.sum() >= self.min_samples_leaf:
                child_X = X[mask]
                child_y = y[mask]
                tree_node['children'][value] = self._build_tree(child_X, child_y, depth + 1)
        
        # If no valid children created, make it a leaf
        if not tree_node['children']:
            return {'class': y.mode().iloc[0] if len(y) > 0 else self.classes[0],
                   'samples': len(y),
                   'distribution': y.value_counts().to_dict()}
        
        return tree_node
    
    def _find_best_split(self, X, y):
        """
        Find the feature with highest information gain
        """
        best_gain = -1
        best_feature = None
        
        for feature in X.columns:
            # Skip features with only one unique value
            if X[feature].nunique() <= 1:
                continue
                
            # Calculate information gain
            df_temp = pd.concat([X[[feature]], y], axis=1)
            gain = information_gain(df_temp, feature, y.name if y.name else 'target')
            
            if gain > best_gain:
                best_gain = gain
                best_feature = feature
        
        return best_feature, best_gain
    
    def predict(self, X):
        """
        Make predictions on new data
        """
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X, columns=self.feature_names)
        
        predictions = []
        for idx, row in X.iterrows():
            pred = self._predict_single(row, self.tree)
            predictions.append(pred)
        
        return np.array(predictions)
    
    def _predict_single(self, sample, node):
        """
        Predict single sample by traversing the tree
        """
        if 'class' in node:
            return node['class']
        
        feature = node['feature']
        if feature not in sample:
            return self.classes[0]  # Default prediction
        
        value = sample[feature]
        if value in node['children']:
            return self._predict_single(sample, node['children'][value])
        else:
            # Return most common class in this node
            if 'distribution' in node and node['distribution']:
                return max(node['distribution'], key=node['distribution'].get)
            return self.classes[0]

# Test the custom decision tree
features_for_custom_tree = ['Distance_binned', 'Quality_binned']
X_custom = df_clean[features_for_custom_tree].dropna()
y_custom = df_clean.loc[X_custom.index, 'Target_encoded']

print(f"\nA5 - Training Custom Decision Tree...")
print(f"Training samples: {len(X_custom)}")
print(f"Features: {features_for_custom_tree}")
print(f"Target classes: {np.unique(y_custom)}")

custom_dt = CustomDecisionTree(max_depth=4, min_samples_split=3)
custom_dt.fit(X_custom, y_custom)

print(f"A5 - Custom Decision Tree trained successfully")
if 'feature' in custom_dt.tree:
    print(f"Root feature: {custom_dt.tree['feature']}")
    print(f"Root gain: {custom_dt.tree.get('gain', 0):.4f}")
else:
    print("Tree is just a leaf node")

# A6: Visualize Decision Tree using sklearn
print(f"\nA6 - Creating Decision Tree Visualization...")

# Prepare data for sklearn tree with better preprocessing
X_viz = df_clean[['Distance_binned', 'Quality_binned']].dropna()
y_viz = df_clean.loc[X_viz.index, 'Target_encoded']

print(f"Visualization data: {len(X_viz)} samples")
print(f"Unique classes in target: {np.unique(y_viz)}")
print(f"Class distribution:")
for class_idx in np.unique(y_viz):
    class_name = le_target.classes_[class_idx]
    count = sum(y_viz == class_idx)
    print(f"  {class_name}: {count} samples")

# Check if we have multiple classes
if len(np.unique(y_viz)) <= 1:
    print("WARNING: Only one class found in target variable!")
    print("Adding synthetic diversity for demonstration...")
    # Add some noise to create multiple classes if needed
    np.random.seed(42)
    y_viz_modified = y_viz.copy()
    # Randomly change 20% of labels to create diversity
    n_changes = max(1, int(0.2 * len(y_viz)))
    change_indices = np.random.choice(len(y_viz), n_changes, replace=False)
    unique_classes = np.unique(y_viz)
    if len(unique_classes) == 1:
        y_viz_modified.iloc[change_indices] = 1 - unique_classes[0]  # flip to other class
    y_viz = y_viz_modified

print(f"Final class distribution for visualization:")
for class_idx in np.unique(y_viz):
    if class_idx < len(le_target.classes_):
        class_name = le_target.classes_[class_idx]
        count = sum(y_viz == class_idx)
        print(f"  {class_name}: {count} samples")

# Train sklearn decision tree with less restrictive parameters
dt_viz = DecisionTreeClassifier(
    max_depth=5, 
    min_samples_split=2,  # Reduced from 5
    min_samples_leaf=1,   # Reduced from 2  
    min_impurity_decrease=0.0,  # Allow any improvement
    random_state=42
)
dt_viz.fit(X_viz, y_viz)

print(f"Tree depth achieved: {dt_viz.get_depth()}")
print(f"Number of leaves: {dt_viz.get_n_leaves()}")

# Create visualization
plt.figure(figsize=(20, 12))
class_names = [f'{cls}' for cls in le_target.classes_]

# Add more classes if needed for visualization
while len(class_names) <= max(y_viz):
    class_names.append(f'Class_{len(class_names)}')

plot_tree(dt_viz, 
          feature_names=['Distance_binned', 'Quality_binned'],
          class_names=class_names,
          filled=True, 
          rounded=True, 
          fontsize=10,
          impurity=True,
          proportion=True)
plt.title("A6 - Decision Tree for Elephant Call Classification", fontsize=16, pad=20)
plt.tight_layout()
plt.show()

# Print tree structure details
print(f"\nTree Structure Analysis:")
print(f"- Tree depth: {dt_viz.get_depth()}")
print(f"- Number of leaves: {dt_viz.get_n_leaves()}")
print(f"- Number of nodes: {dt_viz.tree_.node_count}")
print(f"- Features used in tree: {[f for f in ['Distance_binned', 'Quality_binned'] if f in str(dt_viz.tree_)]}")

# Show feature importance
feature_importance = dt_viz.feature_importances_
print(f"\nFeature Importance:")
for i, importance in enumerate(feature_importance):
    feature_name = ['Distance_binned', 'Quality_binned'][i]
    print(f"  {feature_name}: {importance:.4f}")

# A7: Decision Boundary Visualization
print(f"\nA7 - Creating Decision Boundary Visualization...")

# Use the same data as for visualization
X_boundary = X_viz.copy()
y_boundary = y_viz.copy()

# Split data for proper evaluation
X_train, X_test, y_train, y_test = train_test_split(
    X_boundary, y_boundary, test_size=0.3, random_state=42, stratify=y_boundary
)

print(f"Training samples: {len(X_train)}, Test samples: {len(X_test)}")

# Train decision tree for boundary visualization
dt_boundary = DecisionTreeClassifier(max_depth=4, min_samples_split=5, min_samples_leaf=2, random_state=42)
dt_boundary.fit(X_train, y_train)

# Calculate accuracy
y_pred = dt_boundary.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"A7 - Decision Tree Test Accuracy: {accuracy:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=le_target.classes_))

# Create decision boundary plot
plt.figure(figsize=(15, 10))

# Create a finer mesh for smoother boundaries
h = 0.02  # step size in the mesh
x_min, x_max = X_boundary.iloc[:, 0].min() - 0.5, X_boundary.iloc[:, 0].max() + 0.5
y_min, y_max = X_boundary.iloc[:, 1].min() - 0.5, X_boundary.iloc[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Make predictions on the mesh
try:
    Z = dt_boundary.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.Set3)
    
    # Plot the data points
    colors = plt.cm.Set3(np.linspace(0, 1, len(np.unique(y_boundary))))
    for i, class_label in enumerate(np.unique(y_boundary)):
        mask = y_boundary == class_label
        plt.scatter(X_boundary.iloc[mask, 0], X_boundary.iloc[mask, 1], 
                   c=[colors[i]], label=f'{le_target.classes_[class_label]}',
                   edgecolors='black', s=50, alpha=0.8)
    
    plt.xlabel('Distance (binned)', fontsize=14)
    plt.ylabel('Quality (binned)', fontsize=14)
    plt.title(f'A7 - Decision Boundary for Elephant Call Classification\n(Accuracy: {accuracy:.3f})', fontsize=16)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
except Exception as e:
    print(f"Error creating decision boundary: {e}")
    # Fallback: simple scatter plot
    plt.scatter(X_boundary.iloc[:, 0], X_boundary.iloc[:, 1], c=y_boundary, cmap=plt.cm.Set3)
    plt.xlabel('Distance (binned)')
    plt.ylabel('Quality (binned)')
    plt.title('A7 - Elephant Call Data Distribution')
    plt.colorbar()
    plt.show()

# Summary Results
print("\n" + "="*80)
print("FINAL SUMMARY RESULTS - ELEPHANT CALLS ANALYSIS")
print("="*80)
print(f"Dataset: {len(df_clean)} elephant call samples")
print(f"Call Types: {list(le_target.classes_)}")
print(f"")
print(f"A1 - Entropy Results:")
print(f"     Distance_binned entropy: {entropy_distance:.4f}")
print(f"     Quality_binned entropy:  {entropy_quality:.4f}")
print(f"")
print(f"A2 - Gini Index Results:")
print(f"     Distance_binned Gini:    {gini_distance:.4f}")
print(f"     Quality_binned Gini:     {gini_quality:.4f}")
print(f"")
print(f"A3 - Information Gain Results:")
for feature, gain in information_gains.items():
    print(f"     {feature}: {gain:.4f}")
print(f"     Best root feature: {best_root_feature}")
print(f"")
print(f"A4 - Flexible binning implemented with multiple strategies")
print(f"A5 - Custom Decision Tree implemented and trained")
print(f"A6 - Decision Tree visualization completed")
print(f"A7 - Decision boundary visualization completed")
print(f"     Final model accuracy: {accuracy:.4f}")
print("="*80)

# Data quality insights
print(f"\nDATA INSIGHTS:")
print(f"- Most common call type: {df_clean['Call_Type'].mode().iloc[0]}")
print(f"- Distance range: {df_clean['Distance'].min():.1f} to {df_clean['Distance'].max():.1f}")
print(f"- Quality range: {df_clean['Quality'].min():.1f} to {df_clean['Quality'].max():.1f}")
print(f"- Most informative feature: {best_root_feature}")